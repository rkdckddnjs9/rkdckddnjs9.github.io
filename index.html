<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>MAESTRO â€” Multi-task 3D Perception</title>
  <meta name="description" content="MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception." />
  <style>
    :root { --accent:#0a84ff; --text:#111; --muted:#666; --bg:#fff; --chip:#f5f7fb; }
    *{box-sizing:border-box}
    html,body{margin:0;padding:0;background:var(--bg);color:var(--text);font:16px/1.6 system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,"Apple Color Emoji","Segoe UI Emoji"}
    a{color:var(--accent);text-decoration:none}
    a:hover{text-decoration:underline}
    .wrap{max-width:980px;margin:0 auto;padding:24px}
    
/*     header{padding:24px 0 12px} */
    header {
      padding:24px 0 12px;
      text-align: center;   /* ì¤‘ì•™ ì •ë ¬ */
    }
    
    h1{font-size:clamp(24px,3.5vw,40px);margin:0 0 8px;line-height:1.2}
    .authors{display:flex;flex-wrap:wrap;gap:8px 16px;color:var(--muted);font-size:15px;justify-content: center; /* ì¤‘ì•™ì •ë ¬ ì¶”ê°€ */}
    .affil{color:var(--muted);font-size:14px;margin-top:6px;justify-content: center; /* ì¤‘ì•™ì •ë ¬ ì¶”ê°€ */}
    .chips{display:flex;flex-wrap:wrap;gap:10px;margin:16px 0 8px;justify-content: center; /* ì¤‘ì•™ì •ë ¬ ì¶”ê°€ */}
    .chip{background:var(--chip);border:1px solid #e6ebf5;border-radius:999px;padding:8px 14px;font-weight:600;display:inline-flex;gap:8px;align-items:center}
    .teaser{margin:18px 0;}
    h2{margin:26px 0 10px;font-size:22px}
    p{margin:10px 0}

    /* Figure ìŠ¤íƒ€ì¼ */
    .figure-img {
      width:100%;
      height:auto;
      display:block;
      margin:0 auto;
      border-radius:12px;
      border:1px solid #eee;
    }
    .figure-cap {
      margin-top:8px;
      font-size:15px;
      color:#555;
      text-align:center;
    }

    /* ê°¤ëŸ¬ë¦¬ ìŠ¤íƒ€ì¼ */
    .grid {
      display:grid;
      grid-template-columns:repeat(auto-fit,minmax(300px,1fr));
      gap:20px;
    }
    .card {
      border:1px solid #eee;
      border-radius:12px;
      overflow:hidden;
      background:#fff;
    }
    .card img {
      width:100%;
      height:auto;        /* ì›ë³¸ ë¹„ìœ¨ ìœ ì§€ */
      max-height:500px;   /* ë„ˆë¬´ í° ê²½ìš° ì œí•œ */
      object-fit:contain; /* ë¹„ìœ¨ ìœ ì§€í•˜ë©° ì¶•ì†Œ */
      display:block;
    }
    .card .cap {
      padding:10px 12px;
      font-size:14px;
      color:var(--muted);
      text-align:center;
    }

    /* Swiper ìŠ¬ë¼ì´ë“œ í†µì¼ */
    .swiper {
      width: 100%;
      max-width: 900px;  /* í•„ìš”í•˜ë©´ ìŠ¬ë¼ì´ë“œ ìµœëŒ€ í­ ì œí•œ */
      margin: 0 auto;
    }
    .swiper-slide {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 400px;      /* ìŠ¬ë¼ì´ë“œ ë†’ì´ ê³ ì • */
      background: #fff;
    }
    .swiper-slide img {
      max-height: 100%;
      max-width: 100%;
      object-fit: contain; /* ë¹„ìœ¨ ìœ ì§€ */
      display: block;
    }

    h2 {
      margin: 26px 0 10px;
      font-size: 26px;     /* ì œëª© í¬ê¸° í‚¤ì›€ */
      text-align: center;  /* ì œëª© ì¤‘ì•™ì •ë ¬ */
    }
  
    section p {
      text-align: center;  /* ë³¸ë¬¸ ì¤‘ì•™ì •ë ¬ */
      font-size: 18px;     /* ë³¸ë¬¸ í¬ê¸° í™•ëŒ€ */
      line-height: 1.8;
    }
  
    /* Abstract & Overviewë§Œ ë”°ë¡œ ì¡°ì •í•˜ê³  ì‹¶ì„ ë•Œ */
    #abstract p, #overall-architecture p {
      font-size: 18px;
      text-align: center;
    }

    #abstract p, 
    #overall-architecture p {
      text-align: left;   /* ì¢Œì¸¡ ì •ë ¬ */
      font-size: 18px;    /* í¬ê¸°ëŠ” ê·¸ëŒ€ë¡œ */
      line-height: 1.8;
    }

    pre{background:#0f172a;color:#e5e7eb;padding:14px;border-radius:10px;overflow:auto}
    footer{color:var(--muted);font-size:14px;margin:28px 0 8px}
    .btns{display:flex;flex-wrap:wrap;gap:10px}
    .btn{display:inline-flex;align-items:center;gap:8px;padding:10px 14px;border-radius:10px;border:1px solid #e6ebf5;background:#fff;font-weight:600}
    .btn:hover{background:#f8fafc}
    .license{font-size:13px;color:var(--muted)}
  </style>
  <!-- Swiper CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.css" />
</head>
<body>
  <div class="wrap">
    <header>
      <h1>MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception</h1>
      <div class="authors">
        <span>Changwon Kang*</span><span> Jisong Kim*</span><span> Hongjae Shin*</span><span> Junseo Park</span><span> Jun Won Choiâ€ </span>
      </div>
      <div class="affil">ADR Lab / Hanyang University & Seoul National University â€¢ Seoul, Korea</div>
      <div class="chips">
        <a class="chip" href="https://arxiv.org/pdf/2509.17462" target="_blank" rel="noopener">ğŸ“„ Paper</a>
        <a class="chip" id="code" href="https://github.com/rkdckddnjs9/MAESTRO" target="_blank" rel="noopener">ğŸ’» Code (GitHub)</a>
        <a class="chip" href="#bibtex">ğŸ”– BibTeX</a>
      </div>
    </header>

    <!-- Teaser -->
<!--     <section id="teaser">
      <figure style="text-align:center; margin:24px 0;">
        <img src="github_images/mtl_comparison_v9.png" 
             alt="Teaser figure for MAESTRO method"
             class="figure-img" />
        <figcaption class="figure-cap">
          <strong>Figure 1.</strong> Comparison of Single-Task and Multi-Task 3D Perception Frameworks. (a) Single-task method: Each task utilizes a dedicated backbone and task-specific features, ensuring task specialization at the expense of high computational cost. (b) Existing multi-task method: A shared backbone improves computational efficiency but lacks task-specific feature representation. (c) Proposed method: Task-specific features are generated from shared backbone features, achieving both efficiency and task-aware feature learning.
        </figcaption>
      </figure>
    </section> -->

    <!-- Abstract -->
    <section id="abstract">
      <h2>Abstract</h2>
      <p>
        The goal of multi-task learning is to learn to conduct multiple tasks simultaneously based on a shared data representation. While this approach can improve learning efficiency, it may also cause performance degradation due to task conflicts that arise when optimizing the model for different objectives.
        To address this challenge, we introduce MAESTRO, a structured framework designed to generate task-specific features and mitigate feature interference in multi-task 3D perception, including 3D object detection, bird's-eye view (BEV) map segmentation, and 3D occupancy prediction.
        MAESTRO comprises three components: the Class-wise Prototype Generator (CPG), the Task-Specific Feature Generator (TSFG), and the Scene Prototype Aggregator (SPA).
        CPG groups class categories into foreground and background groups and generates group-wise prototypes. The foreground and background prototypes are assigned to the 3D object detection task and the map segmentation task, respectively, while both are assigned to the 3D occupancy prediction task. TSFG leverages these prototype groups to retain task-relevant features while suppressing irrelevant features, thereby enhancing the performance for each task. SPA enhances the prototype groups assigned for 3D occupancy prediction by utilizing the information produced by the 3D object detection head and the map segmentation head. Extensive experiments on the nuScenes and Occ3D benchmarks demonstrate that MAESTRO consistently outperforms existing methods across 3D object detection, BEV map segmentation, and 3D occupancy prediction tasks.
      </p>
    </section>

    <!-- Overall Architecture -->
    <section id="overall-architecture">
      <h2>Overview</h2>
      <figure style="text-align:center; margin:24px 0;">
        <img src="github_images/Overall_Architecture_v2.png" 
             alt="Overall architecture of MAESTRO"
             class="figure-img" />
        <figcaption class="figure-cap">
          <strong>Figure 1.</strong> Overall architecture of the proposed MAESTRO framework with CPG, TSFG, and SPA modules.
        </figcaption>
      </figure>

      <p>
        Multi-view images are processed by a shared backbone to generate a structured 3D voxel representation. The CPG generates foreground and background prototype groups, which guide the TSFG in refining task-relevant features for 3D object detection, BEV map segmentation, and 3D occupancy prediction. Task-oriented prototypes derived from the 3D object detection and BEV map segmentation heads are then integrated with the prototype groups via the SPA, forming Scene Prototypes. These Scene prototypes are subsequently processed by the occupancy decoder to produce the final 3D occupancy predictions.
      </p>
    </section>

    <!-- Detailed Architecture -->
    <section id="Detailed Architecture of TSFG">
      <h2>Detailed Architecture of TSFG</h2>
      <figure style="text-align:center; margin:24px 0;">
<!--         <img src="github_images/TSFG_v5.png" 
             alt="Detailed architecture of TSFG"
             class="figure-img" /> -->
        <img src="github_images/TSFG_v5.png" 
         alt="Detailed architecture of TSFG"
         class="figure-img"
         style="width:70%; height:auto;" />
        <figcaption class="figure-cap">
          <strong>Figure 2.</strong> Detailed structure of TSFG. TSFG refines task-relevant features by leveraging the prototype group, followed by feature suppression to mitigate task-irrelevant information, resulting in task-specific features for each task.
        </figcaption>
      </figure>
    </section>

    <!-- Examples -->
    <section id="gallery">
      <h2>Qualitative Results</h2>
      <div class="grid">
        <figure class="card">
          <img src="github_images/main_viz.png" alt="Example 1" />
          <figcaption class="cap">Qualitative results on the nuScenes validation set. From left to right, we show Ground Truth, Baseline-MTL, and our MAESTRO.</figcaption>
        </figure>
      </div>
    </section>

    <section id="gallery">
      <h2>Quantitative Results</h2>
      <div class="grid">
        <figure class="card">
          <img src="github_images/main_table.png" alt="Example 1" />
          <figcaption class="cap">Performance comparison evaluated on the nuScenes validation set. ``-" indicates tasks not supported by the method. All BEV map segmentation and MTL results were reproduced using official code with ResNet-50 backbone. ``*" denotes the performance reported by DualBEV. ``â€ " indicates the method using CBGS for training. Our MTL approach achieves state-of-the-art performance across all tasks.</figcaption>
        </figure>
      </div>
    </section>


    <!-- Extended Qualitative Results (Swiper) -->
    <section id="Additional Qualitative Results">
      <h2>Additional Qualitative Results</h2>
      <div class="swiper">
        <div class="swiper-wrapper">
          <div class="swiper-slide"><img src="github_images/weather_v1_1.png" /></div>
          <div class="swiper-slide"><img src="github_images/weather_v1_2.png" /></div>
          <div class="swiper-slide"><img src="github_images/supply_1.png" /></div>
          <div class="swiper-slide"><img src="github_images/Supply_BEVSeg_Comparison_v2.png" /></div>
        </div>
        <!-- ë„¤ë¹„ê²Œì´ì…˜ ë²„íŠ¼ -->
        <div class="swiper-button-next"></div>
        <div class="swiper-button-prev"></div>
      </div>
    </section>

    <!-- Swiper JS -->
    <script src="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.js"></script>
    <script>
      const swiper = new Swiper('.swiper', {
        loop: true,
        navigation: {
          nextEl: '.swiper-button-next',
          prevEl: '.swiper-button-prev'
        }
      });
    </script>

    <!-- Citation -->
    <section id="bibtex">
      <h2>Citation</h2>
      <pre>@article{kang2025maestro,
  title={MAESTRO: Task-Relevant Optimization via Adaptive Feature Enhancement and Suppression for Multi-task 3D Perception},
  author={Kang, Changwon and Kim, Jisong and Shin, Hongjae and Park, Junseo and Choi, Jun Won},
  journal={arXiv preprint arXiv:2509.17462},
  year={2025}
}</pre>
    </section>

    <!-- License -->
    <section id="license">
      <h2>License & Credits</h2>
      <p class="license">
        This pageâ€™s HTML/CSS scaffold is adapted from an open academic project template (Nerfies-style). If you re-use this, keep attribution in the footer and remove any analytics snippets you do not intend to use.
      </p>
    </section>

    <footer>
      <div>Â© 2025 Your Lab â€¢ CC BY-SA for the website content.</div>
      <div>Built with a minimal static page; deploy via GitHub Pages or any static host.</div>
    </footer>
  </div>
</body>
</html>
